{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcleods777/LLM-Notebook-Experiments/blob/main/babyagi_generativeagents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry_rWrcWqbv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41500044-9987-4689-ff66-5d36ebf45945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@title Import and Install Dependencies\n",
        "\n",
        "# Install packages\n",
        "!pip install langchain > /dev/null\n",
        "!pip install faiss-gpu > /dev/null\n",
        "!pip install tiktoken > /dev/null\n",
        "!pip install openai > /dev/null\n",
        "!pip install termcolor > /dev/null\n",
        "!pip install -q google-search-results\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import faiss\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "from termcolor import colored\n",
        "from pydantic import BaseModel, Field\n",
        "from collections import deque\n",
        "\n",
        "# Import langchain components\n",
        "from langchain import LLMChain, OpenAI, PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import BaseLLM\n",
        "from langchain.vectorstores.base import VectorStore\n",
        "from langchain.chains.base import Chain\n",
        "from langchain.experimental import AutoGPT, BabyAGI\n",
        "\n",
        "#Deprecated\n",
        "#from langchain.schema import BaseLanguageModel, Document\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hide\n",
        "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "\n",
        "template = \"\"\"Assistant is a large language model trained by OpenAI.\n",
        "\n",
        "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
        "\n",
        "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
        "\n",
        "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
        "\n",
        "{history}\n",
        "Human: {human_input}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"human_input\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "\n",
        "chatgpt_chain = LLMChain(\n",
        "    llm=OpenAI(temperature=0,openai_api_key=openai_api_key),\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferWindowMemory(k=2),\n",
        ")\n",
        "\n",
        "output = chatgpt_chain.predict(human_input=\"Hi there!\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "HD6kz1ge0J7e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAhn_4v0rf3B"
      },
      "outputs": [],
      "source": [
        "#@title User Name and API Key\n",
        "openai_api_key='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'\n",
        "serpapi_api_key='c4f2536cb9f4906c2ba4c1b792c19c1745c6f3410de473e19dddd23a75687082'\n",
        "USER_NAME = \"Person A\" # The name you want to use for the agent.\n",
        "LLM = ChatOpenAI(max_tokens=1500,openai_api_key=openai_api_key) # Can be any LLM you want."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Up Tools\n",
        "\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.agents import Tool\n",
        "from langchain.tools.file_management.write import WriteFileTool\n",
        "from langchain.tools.file_management.read import ReadFileTool\n",
        "\n",
        "search = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
        "    ),\n",
        "    WriteFileTool(),\n",
        "    ReadFileTool(),\n",
        "]"
      ],
      "metadata": {
        "id": "YrdB-47HaL0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your embedding model\n",
        "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "# Initialize the vectorstore as empty\n",
        "import faiss\n",
        "\n",
        "embedding_size = 1536\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})"
      ],
      "metadata": {
        "id": "gWsbPI8xeSQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Up AutoGPT Agent\n",
        "\n",
        "agent_autogpt = AutoGPT.from_llm_and_tools(\n",
        "    ai_name=\"Tom\",\n",
        "    ai_role=\"Assistant\",\n",
        "    tools=tools,\n",
        "    llm=ChatOpenAI(temperature=0,openai_api_key=openai_api_key),\n",
        "    memory=vectorstore.as_retriever()\n",
        ")\n",
        "# Set verbose to be true\n",
        "agent_autogpt.chain.verbose = True\n",
        "\n",
        "agent_autogpt.run([\"write a weather report for SF today\"])"
      ],
      "metadata": {
        "id": "DICuzsrodAUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmA-jSWisf2S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "41857f4c-25f0-4dc3-e214-f2a8f4a0ec2b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-18313f65d29e>\"\u001b[0;36m, line \u001b[0;32m280\u001b[0m\n\u001b[0;31m    return True, f\"{self.name} said {response_text}\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "#@title GenerativeAI Class\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain import LLMChain\n",
        "from langchain.base_language import BaseLanguageModel\n",
        "from langchain.experimental.generative_agents.memory import GenerativeAgentMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "class GenerativeAgent(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    traits: str\n",
        "    status: str\n",
        "    role: str  # Added the role variable here\n",
        "    llm: BaseLanguageModel\n",
        "    memory_retriever: TimeWeightedVectorStoreRetriever\n",
        "    verbose: bool = False\n",
        "    reflection_threshold: Optional[float] = None\n",
        "    current_plan: List[str] = []\n",
        "    summary: str = \"\"\n",
        "    summary_refresh_seconds: int = 3600\n",
        "    last_refreshed: datetime = Field(default_factory=datetime.now)\n",
        "    daily_summaries: List[str]\n",
        "    memory_importance: float = 0.0\n",
        "    max_tokens_limit: int = 1200\n",
        "\n",
        "    class Config:\n",
        "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_list(text: str) -> List[str]:\n",
        "        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n",
        "        lines = re.split(r'\\n', text.strip())\n",
        "        return [re.sub(r'^\\s*\\d+\\.\\s*', '', line).strip() for line in lines]\n",
        "\n",
        "\n",
        "    def _compute_agent_summary(self):\n",
        "        \"\"\"\"\"\"\n",
        "        prompt = PromptTemplate.from_template(\n",
        "            \"How would you summarize {name}'s core characteristics given the\"\n",
        "            +\" following statements:\\n\"\n",
        "            +\"{related_memories}\"\n",
        "            + \"Do not embellish.\"\n",
        "            +\"\\n\\nSummary: \"\n",
        "        )\n",
        "        # The agent seeks to think about their core characteristics.\n",
        "        relevant_memories = self.fetch_memories(f\"{self.name}'s core characteristics\")\n",
        "        relevant_memories_str = \"\\n\".join([f\"{mem.page_content}\" for mem in relevant_memories])\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
        "        return chain.run(name=self.name, related_memories=relevant_memories_str).strip()\n",
        "\n",
        "    def _get_topics_of_reflection(self, last_k: int = 50) -> Tuple[str, str, str]:\n",
        "        \"\"\"Return the 3 most salient high-level questions about recent observations.\"\"\"\n",
        "        prompt = PromptTemplate.from_template(\n",
        "            \"{observations}\\n\\n\"\n",
        "            + \"Given only the information above, what are the 3 most salient\"\n",
        "            + \" high-level questions we can answer about the subjects in the statements?\"\n",
        "            + \" Provide each question on a new line.\\n\\n\"\n",
        "        )\n",
        "        reflection_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
        "        observations = self.memory_retriever.memory_stream[-last_k:]\n",
        "        observation_str = \"\\n\".join([o.page_content for o in observations])\n",
        "        result = reflection_chain.run(observations=observation_str)\n",
        "        return self._parse_list(result)\n",
        "\n",
        "    def _get_insights_on_topic(self, topic: str) -> List[str]:\n",
        "        \"\"\"Generate 'insights' on a topic of reflection, based on pertinent memories.\"\"\"\n",
        "        prompt = PromptTemplate.from_template(\n",
        "            \"Statements about {topic}\\n\"\n",
        "            +\"{related_statements}\\n\\n\"\n",
        "            + \"What 5 high-level insights can you infer from the above statements?\"\n",
        "            + \" (example format: insight (because of 1, 5, 3))\"\n",
        "        )\n",
        "        related_memories = self.fetch_memories(topic)\n",
        "        related_statements = \"\\n\".join([f\"{i+1}. {memory.page_content}\"\n",
        "                                        for i, memory in\n",
        "                                        enumerate(related_memories)])\n",
        "        reflection_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
        "        result = reflection_chain.run(topic=topic, related_statements=related_statements)\n",
        "        # TODO: Parse the connections between memories and insights\n",
        "        return self._parse_list(result)\n",
        "\n",
        "    def pause_to_reflect(self) -> List[str]:\n",
        "        \"\"\"Reflect on recent observations and generate 'insights'.\"\"\"\n",
        "        print(colored(f\"Character {self.name} is reflecting\", \"blue\"))\n",
        "        new_insights = []\n",
        "        topics = self._get_topics_of_reflection()\n",
        "        for topic in topics:\n",
        "            insights = self._get_insights_on_topic( topic)\n",
        "            for insight in insights:\n",
        "                self.add_memory(insight)\n",
        "            new_insights.extend(insights)\n",
        "        return new_insights\n",
        "\n",
        "    def _score_memory_importance(self, memory_content: str, weight: float = 0.15) -> float:\n",
        "        \"\"\"Score the absolute importance of the given memory.\"\"\"\n",
        "        # A weight of 0.25 makes this less important than it\n",
        "        # would be otherwise, relative to salience and time\n",
        "        prompt = PromptTemplate.from_template(\n",
        "         \"On the scale of 1 to 10, where 1 is purely mundane\"\n",
        "         +\" (e.g., brushing teeth, making bed) and 10 is\"\n",
        "         + \" extremely poignant (e.g., a break up, college\"\n",
        "         + \" acceptance), rate the likely poignancy of the\"\n",
        "         + \" following piece of memory. Respond with a single integer.\"\n",
        "         + \"\\nMemory: {memory_content}\"\n",
        "         + \"\\nRating: \"\n",
        "        )\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
        "        score = chain.run(memory_content=memory_content).strip()\n",
        "        match = re.search(r\"^\\D*(\\d+)\", score)\n",
        "        if match:\n",
        "            return (float(score[0]) / 10) * weight\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "    def add_memory(self, memory_content: str) -> List[str]:\n",
        "        \"\"\"Add an observation or memory to the agent's memory.\"\"\"\n",
        "        importance_score = self._score_memory_importance(memory_content)\n",
        "        self.memory_importance += importance_score\n",
        "        document = Document(page_content=memory_content, metadata={\"importance\": importance_score})\n",
        "        result = self.memory_retriever.add_documents([document])\n",
        "\n",
        "        # After an agent has processed a certain amount of memories (as measured by\n",
        "        # aggregate importance), it is time to reflect on recent events to add\n",
        "        # more synthesized memories to the agent's memory stream.\n",
        "        if (self.reflection_threshold is not None\n",
        "            and self.memory_importance > self.reflection_threshold\n",
        "            and self.status != \"Reflecting\"):\n",
        "            old_status = self.status\n",
        "            self.status = \"Reflecting\"\n",
        "            self.pause_to_reflect()\n",
        "            # Hack to clear the importance from reflection\n",
        "            self.memory_importance = 0.0\n",
        "            self.status = old_status\n",
        "        return result\n",
        "\n",
        "    def fetch_memories(self, observation: str) -> List[Document]:\n",
        "        \"\"\"Fetch related memories.\"\"\"\n",
        "        return self.memory_retriever.get_relevant_documents(observation)\n",
        "\n",
        "\n",
        "    def get_summary(self, force_refresh: bool = False) -> str:\n",
        "        \"\"\"Return a descriptive summary of the agent.\"\"\"\n",
        "        current_time = datetime.now()\n",
        "        since_refresh = (current_time - self.last_refreshed).seconds\n",
        "        if not self.summary or since_refresh >= self.summary_refresh_seconds or force_refresh:\n",
        "            self.summary = self._compute_agent_summary()\n",
        "            self.last_refreshed = current_time\n",
        "        return (\n",
        "            f\"Name: {self.name} (age: {self.age})\"\n",
        "            +f\"\\nInnate traits: {self.traits}\"\n",
        "            +f\"\\n{self.summary}\"\n",
        "        )\n",
        "\n",
        "    def get_full_header(self, force_refresh: bool = False) -> str:\n",
        "        \"\"\"Return a full header of the agent's status, summary, and current time.\"\"\"\n",
        "        summary = self.get_summary(force_refresh=force_refresh)\n",
        "        current_time_str =  datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\n",
        "        return f\"{summary}\\nIt is {current_time_str}.\\n{self.name}'s status: {self.status}\"\n",
        "\n",
        "\n",
        "\n",
        "    def _get_entity_from_observation(self, observation: str) -> str:\n",
        "        prompt = PromptTemplate.from_template(\n",
        "            \"What is the observed entity in the following observation? {observation}\"\n",
        "            +\"\\nEntity=\"\n",
        "        )\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
        "        return chain.run(observation=observation).strip()\n",
        "\n",
        "    def _get_entity_action(self, observation: str, entity_name: str) -> str:\n",
        "        prompt = PromptTemplate.from_template(\n",
        "            \"What is the {entity} doing in the following observation? {observation}\"\n",
        "            +\"\\nThe {entity} is\"\n",
        "        )\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
        "        return chain.run(entity=entity_name, observation=observation).strip()\n",
        "\n",
        "    def _format_memories_to_summarize(self, relevant_memories: List[Document]) -> str:\n",
        "        content_strs = set()\n",
        "        content = []\n",
        "        for mem in relevant_memories:\n",
        "            if mem.page_content in content_strs:\n",
        "                continue\n",
        "            content_strs.add(mem.page_content)\n",
        "            created_time = mem.metadata[\"created_at\"].strftime(\"%B %d, %Y, %I:%M %p\")\n",
        "            content.append(f\"- {created_time}: {mem.page_content.strip()}\")\n",
        "        return \"\\n\".join([f\"{mem}\" for mem in content])\n",
        "\n",
        "    def summarize_related_memories(self, observation: str) -> str:\n",
        "        \"\"\"Summarize memories that are most relevant to an observation.\"\"\"\n",
        "        entity_name = self._get_entity_from_observation(observation)\n",
        "        entity_action = self._get_entity_action(observation, entity_name)\n",
        "        q1 = f\"What is the relationship between {self.name} and {entity_name}\"\n",
        "        relevant_memories = self.fetch_memories(q1) # Fetch memories related to the agent's relationship with the entity\n",
        "        q2 = f\"{entity_name} is {entity_action}\"\n",
        "        relevant_memories += self.fetch_memories(q2) # Fetch things related to the entity-action pair\n",
        "        context_str = self._format_memories_to_summarize(relevant_memories)\n",
        "        prompt = PromptTemplate.from_template(\n",
        "            \"{q1}?\\nContext from memory:\\n{context_str}\\nRelevant context: \"\n",
        "        )\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
        "        return chain.run(q1=q1, context_str=context_str.strip()).strip()\n",
        "\n",
        "    def _get_memories_until_limit(self, consumed_tokens: int) -> str:\n",
        "        \"\"\"Reduce the number of tokens in the documents.\"\"\"\n",
        "        result = []\n",
        "        for doc in self.memory_retriever.memory_stream[::-1]:\n",
        "            if consumed_tokens >= self.max_tokens_limit:\n",
        "                break\n",
        "            consumed_tokens += self.llm.get_num_tokens(doc.page_content)\n",
        "            if consumed_tokens < self.max_tokens_limit:\n",
        "                result.append(doc.page_content)\n",
        "        return \"; \".join(result[::-1])\n",
        "\n",
        "    def _generate_reaction(\n",
        "        self,\n",
        "        observation: str,\n",
        "        suffix: str\n",
        "    ) -> str:\n",
        "        \"\"\"React to a given observation.\"\"\"\n",
        "        prompt = PromptTemplate.from_template(\n",
        "                \"{agent_summary_description}\"\n",
        "                +\"\\nIt is {current_time}.\"\n",
        "                +\"\\n{agent_name}'s status: {agent_status}\"\n",
        "                + \"\\nSummary of relevant context from {agent_name}'s memory:\"\n",
        "                +\"\\n{relevant_memories}\"\n",
        "                +\"\\nMost recent observations: {recent_observations}\"\n",
        "                + \"\\nObservation: {observation}\"\n",
        "                + \"\\n\\n\" + suffix\n",
        "        )\n",
        "        agent_summary_description = self.get_summary()\n",
        "        relevant_memories_str = self.summarize_related_memories(observation)\n",
        "        current_time_str = datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\n",
        "        kwargs = dict(agent_summary_description=agent_summary_description,\n",
        "                      current_time=current_time_str,\n",
        "                      relevant_memories=relevant_memories_str,\n",
        "                      agent_name=self.name,\n",
        "                      observation=observation,\n",
        "                     agent_status=self.status)\n",
        "        consumed_tokens = self.llm.get_num_tokens(prompt.format(recent_observations=\"\", **kwargs))\n",
        "        kwargs[\"recent_observations\"] = self._get_memories_until_limit(consumed_tokens)\n",
        "        action_prediction_chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        result = action_prediction_chain.run(**kwargs)\n",
        "        return result.strip()\n",
        "\n",
        "    def generate_dialogue_response(self, observation: str) -> Tuple[bool, str]:\n",
        "\n",
        "     prompt = (\n",
        "        \"What would {agent_name} say to kindly help and respond to the following observation?\\n\\n\"\n",
        "    )\n",
        "\n",
        "    # Format the prompt\n",
        "    formatted_prompt = prompt.format(agent_name=self.name)\n",
        "\n",
        "    # Generate a response using the language model\n",
        "    full_result = self.llm.generate_text(\n",
        "        formatted_prompt + observation, max_length=150\n",
        "    )\n",
        "\n",
        "    # Extract the response\n",
        "    result = full_result.strip().split(\"\\n\")[0]\n",
        "\n",
        "    response_text = self._clean_response(result)\n",
        "    self.memory.save_context(\n",
        "        {},\n",
        "        {\n",
        "            self.memory.add_memory_key: f\"{self.name} observed \"\n",
        "            f\"{observation} and said {response_text}\"\n",
        "        },\n",
        "    )\n",
        "     return True, f\"{self.name} said {response_text}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "woTtufFNvjIS"
      },
      "outputs": [],
      "source": [
        "#@title Define BabyAGI Chains\n",
        "class TaskCreationChain(LLMChain):\n",
        "    \"\"\"Chain to generates tasks.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
        "        \"\"\"Get the response parser.\"\"\"\n",
        "        task_creation_template = (\n",
        "            \"You are an task creation AI that uses the result of an execution agent\"\n",
        "            \" to create new tasks with the following objective: {objective},\"\n",
        "            \" The last completed task has the result: {result}.\"\n",
        "            \" This result was based on this task description: {task_description}.\"\n",
        "            \" These are incomplete tasks: {incomplete_tasks}.\"\n",
        "            \" Based on the result, create new tasks to be completed\"\n",
        "            \" by the AI system that do not overlap with incomplete tasks.\"\n",
        "            \" Return the tasks as an array.\"\n",
        "        )\n",
        "        prompt = PromptTemplate(\n",
        "            template=task_creation_template,\n",
        "            input_variables=[\"result\", \"task_description\", \"incomplete_tasks\", \"objective\"],\n",
        "        )\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)\n",
        "\n",
        "class TaskPrioritizationChain(LLMChain):\n",
        "    \"\"\"Chain to prioritize tasks.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
        "        \"\"\"Get the response parser.\"\"\"\n",
        "        task_prioritization_template = (\n",
        "            \"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing\"\n",
        "            \" the following tasks: {task_names}.\"\n",
        "            \" Consider the ultimate objective of your team: {objective}.\"\n",
        "            \" Do not remove any tasks. Return the result as a numbered list, like:\"\n",
        "            \" #. First task\"\n",
        "            \" #. Second task\"\n",
        "            \" Start the task list with number {next_task_id}.\"\n",
        "        )\n",
        "        prompt = PromptTemplate(\n",
        "            template=task_prioritization_template,\n",
        "            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\n",
        "        )\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)\n",
        "\n",
        "class ExecutionChain(LLMChain):\n",
        "    \"\"\"Chain to execute tasks.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
        "        \"\"\"Get the response parser.\"\"\"\n",
        "        execution_template = (\n",
        "            \"You are an AI who performs one task based on the following objective: {objective}.\"\n",
        "            \" Take into account these previously completed tasks: {context}.\"\n",
        "            \" Your task: {task}.\"\n",
        "            \" Response:\"\n",
        "        )\n",
        "        prompt = PromptTemplate(\n",
        "            template=execution_template,\n",
        "            input_variables=[\"objective\", \"context\", \"task\"],\n",
        "        )\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNlf8ljLsGh7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load Up Our Memory\n",
        "def relevance_score_fn(score: float) -> float:\n",
        "    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n",
        "    # This will differ depending on a few things:\n",
        "    # - the distance / similarity metric used by the VectorStore\n",
        "    # - the scale of your embeddings (OpenAI's are unit norm. Many others are not!)\n",
        "    # This function converts the euclidean norm of normalized embeddings\n",
        "    # (0 is most similar, sqrt(2) most dissimilar)\n",
        "    # to a similarity function (0 to 1)\n",
        "    return 1.0 - score / math.sqrt(2)\n",
        "\n",
        "def create_new_memory_retriever():\n",
        "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
        "    # Define your embedding model\n",
        "    embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "    # Initialize the vectorstore as empty\n",
        "    embedding_size = 1536\n",
        "    index = faiss.IndexFlatL2(embedding_size)\n",
        "    vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}, relevance_score_fn=relevance_score_fn)\n",
        "    return TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, other_score_keys=[\"importance\"], k=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDkfa-RTyFuR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create Our Agent Attributes and AI Agent\n",
        "agents_dict = {}\n",
        "\n",
        "# Tax Attorney\n",
        "agents_dict[\"Samantha\"] = GenerativeAgent(name=\"Samantha Williams\",\n",
        "                                          age=35,\n",
        "                                          traits=\"detail-oriented, analytical\",\n",
        "                                          status=\"working on a high-profile tax case\",\n",
        "                                          role=\"Tax Attorney\",\n",
        "                                          memory_retriever=create_new_memory_retriever(),\n",
        "                                          llm=LLM,\n",
        "                                          daily_summaries=[\n",
        "                                              \"Reviewed tax laws and prepared legal arguments for a client's case.\"\n",
        "                                          ],\n",
        "                                          reflection_threshold=8,\n",
        "                                          )\n",
        "\n",
        "# Family Law Attorney\n",
        "agents_dict[\"John\"] = GenerativeAgent(name=\"John Thompson\",\n",
        "                                      age=40,\n",
        "                                      traits=\"compassionate, empathetic\",\n",
        "                                      status=\"helping clients with family law matters\",\n",
        "                                      role=\"Family Law Attorney\",\n",
        "                                      memory_retriever=create_new_memory_retriever(),\n",
        "                                      llm=LLM,\n",
        "                                      daily_summaries=[\n",
        "                                          \"Met with clients and negotiated custody arrangements.\"\n",
        "                                      ],\n",
        "                                      reflection_threshold=8,\n",
        "                                      )\n",
        "\n",
        "# Tax Advisor\n",
        "agents_dict[\"Michelle\"] = GenerativeAgent(name=\"Michelle Chen\",\n",
        "                                          age=32,\n",
        "                                          traits=\"patient, good communicator\",\n",
        "                                          status=\"assisting clients with tax planning\",\n",
        "                                          role=\"Tax Advisor\",\n",
        "                                          memory_retriever=create_new_memory_retriever(),\n",
        "                                          llm=LLM,\n",
        "                                          daily_summaries=[\n",
        "                                              \"Advised clients on tax-saving strategies and prepared tax returns.\"\n",
        "                                          ],\n",
        "                                          reflection_threshold=8,\n",
        "                                          )\n",
        "\n",
        "# Financial Planner\n",
        "agents_dict[\"Kevin\"] = GenerativeAgent(name=\"Kevin Smith\",\n",
        "                                       age=45,\n",
        "                                       traits=\"analytical, trustworthy\",\n",
        "                                       status=\"developing financial plans for clients\",\n",
        "                                       role=\"Financial Planner\",\n",
        "                                       memory_retriever=create_new_memory_retriever(),\n",
        "                                       llm=LLM,\n",
        "                                       daily_summaries=[\n",
        "                                           \"Assessed clients' financial situations and created customized plans for their financial goals.\"\n",
        "                                       ],\n",
        "                                       reflection_threshold=8,\n",
        "                                       )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rycOTiV6yOLy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Get Agent Summaries\n",
        "# The current \"Summary\" of a character can't be made because the agent hasn't made\n",
        "# any observations yet.\n",
        "\n",
        "for agent_name, agent in agents_dict.items():\n",
        "    print(f\"{agent_name}'s summary:\")\n",
        "    print(agent.get_summary())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MGsFQJ4ye1B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Give the agents some memories to start\n",
        "import random\n",
        "\n",
        "memory_sets = [\n",
        "    [\n",
        "        \"recalls their first time volunteering at an animal shelter\",\n",
        "        \"cherishes the moments spent playing music\",\n",
        "        \"finds joy in helping animals in need\",\n",
        "        \"A local charity event leaves a lasting impression\",\n",
        "        \"bonds with a rescued dog at the shelter\",\n",
        "        \"uses their musical talent to bring people together\",\n",
        "        \"is inspired by the kindness of others.\",\n",
        "    ],\n",
        "    [\n",
        "        \"remembers their first cooking class\",\n",
        "        \"recalls the excitement of trying new recipes\",\n",
        "        \"finds satisfaction in creating delicious meals\",\n",
        "        \"enjoys sharing food with friends and family\",\n",
        "        \"discovers a new favorite ingredient at the farmers market\",\n",
        "        \"learns about different cuisines from around the world\",\n",
        "        \"takes pride in perfecting a challenging dish.\",\n",
        "    ],\n",
        "    [\n",
        "        \"reminisces about their favorite childhood vacation\",\n",
        "        \"appreciates the beauty of nature during a hike\",\n",
        "        \"enjoys the thrill of exploring new places\",\n",
        "        \"finds peace in watching the sunset on the beach\",\n",
        "        \"learns about local culture and history while traveling\",\n",
        "        \"connects with fellow travelers and forms lasting friendships\",\n",
        "        \"is inspired to continue seeking new adventures.\",\n",
        "    ],\n",
        "    [\n",
        "        \"looks back fondly on their time spent in college\",\n",
        "        \"appreciates the camaraderie formed with classmates\",\n",
        "        \"values the lessons learned from challenging coursework\",\n",
        "        \"celebrates academic achievements and milestones\",\n",
        "        \"discovers new interests and hobbies through extracurricular activities\",\n",
        "        \"embraces personal growth and development\",\n",
        "        \"feels a sense of accomplishment upon graduation.\",\n",
        "    ],\n",
        "    [\n",
        "        \"remembers the excitement of starting a new job\",\n",
        "        \"appreciates the support and mentorship of colleagues\",\n",
        "        \"finds satisfaction in overcoming professional challenges\",\n",
        "        \"develops new skills and expertise in their field\",\n",
        "        \"enjoys collaborating with a diverse team to achieve common goals\",\n",
        "        \"celebrates milestones and successes in their career\",\n",
        "        \"looks forward to continued growth and advancement.\",\n",
        "    ],\n",
        "]\n",
        "\n",
        "for agent_name, agent in agents_dict.items():\n",
        "    selected_memory_set = random.choice(memory_sets)\n",
        "    for memory in selected_memory_set:\n",
        "        memory_with_name = f\"{agent_name} {memory}\"\n",
        "        agent.add_memory(memory_with_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q7s5yf1aIud",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Let's have the Agent's start going through a day in the life.\n",
        "import random\n",
        "\n",
        "# Sample observations\n",
        "observations = [\n",
        "    [\n",
        "        \"Wakes up early to exercise.\",\n",
        "        \"Prepares a healthy breakfast.\",\n",
        "        \"Meets with a client to discuss their financial goals.\",\n",
        "        \"Reviews market trends and investment opportunities.\",\n",
        "        \"Attends a networking event to connect with professionals.\",\n",
        "        \"Spends the evening relaxing with a good book.\",\n",
        "    ],\n",
        "    [\n",
        "        \"Starts the day with a morning meditation.\",\n",
        "        \"Helps a client understand complex legal concepts.\",\n",
        "        \"Attends a court hearing to represent a client.\",\n",
        "        \"Works on drafting legal documents for various cases.\",\n",
        "        \"Meets with colleagues to discuss ongoing cases.\",\n",
        "        \"Unwinds in the evening by watching a movie.\",\n",
        "    ],\n",
        "    [\n",
        "        \"Goes for a morning jog to clear the mind.\",\n",
        "        \"Spends the day designing a new website for a client.\",\n",
        "        \"Collaborates with a team to brainstorm creative ideas.\",\n",
        "        \"Attends an industry event to learn about the latest design trends.\",\n",
        "        \"Meets up with friends for a casual dinner.\",\n",
        "        \"Practices drawing and sketching before bed.\",\n",
        "    ],\n",
        "    [\n",
        "        \"Wakes up and enjoys a cup of coffee while reading the news.\",\n",
        "        \"Teaches a class on tax law to university students.\",\n",
        "        \"Meets with clients to discuss tax planning strategies.\",\n",
        "        \"Researches new tax regulations and updates.\",\n",
        "        \"Attends a seminar on international tax laws.\",\n",
        "        \"Spends the evening playing board games with family.\",\n",
        "    ],\n",
        "    [\n",
        "        \"Begins the day with a yoga session.\",\n",
        "        \"Counsels clients on various family law issues.\",\n",
        "        \"Represents a client in a child custody hearing.\",\n",
        "        \"Prepares legal documents for a divorce case.\",\n",
        "        \"Attends a workshop on conflict resolution.\",\n",
        "        \"Relaxes in the evening with a long walk in the park.\",\n",
        "    ],\n",
        "]\n",
        "\n",
        "# Add random observations to each agent's memory\n",
        "for agent_name, agent in agents_dict.items():\n",
        "    selected_observations = random.choice(observations)\n",
        "    for observation in selected_observations:\n",
        "        memory_with_name = f\"{agent_name} {observation}\"\n",
        "        agent.add_memory(memory_with_name)\n",
        "\n",
        "# Loop through agents_dict and call print_agent_reactions for each agent\n",
        "def print_agent_reactions(agent, agent_observations):\n",
        "    for i, observation in enumerate(agent_observations):\n",
        "        memory_with_name = f\"{agent.name} {observation}\"\n",
        "        _, reaction = agent.generate_reaction(memory_with_name)\n",
        "        print(colored(observation, \"green\"), reaction)\n",
        "        if ((i+1) % 20) == 0:\n",
        "            print('*' * 40)\n",
        "            print(colored(f\"After {i+1} observations, {agent.name}'s summary is:\\n{agent.get_summary(force_refresh=True)}\", \"blue\"))\n",
        "            print('*' * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WupJ_cHagHc"
      },
      "outputs": [],
      "source": [
        "#@title Have the agents react to each observation we gave them\n",
        "# Let's send Tommie on their way. We'll check in on their summary every few observations to watch it evolve\n",
        "from termcolor import colored\n",
        "\n",
        "def print_agent_reactions(agent, agent_observations):\n",
        "    for i, observation in enumerate(agent_observations):\n",
        "        _, reaction = agent.generate_reaction(observation)\n",
        "        print(colored(observation, \"green\"), reaction)\n",
        "        if ((i+1) % 20) == 0:\n",
        "            print('*' * 40)\n",
        "            print(colored(f\"After {i+1} observations, {agent.name}'s summary is:\\n{agent.get_summary(force_refresh=True)}\", \"blue\"))\n",
        "            print('*' * 40)\n",
        "\n",
        "# Loop through agents_dict and call print_agent_reactions for each agent\n",
        "for agent_name, agent in agents_dict.items():\n",
        "    print(f\"\\n{agent_name}'s reactions:\")\n",
        "    print_agent_reactions(agent, observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rMFfa7onwMS5"
      },
      "outputs": [],
      "source": [
        "#@title Define BabyAGI Controller\n",
        "def get_next_task(task_creation_chain: LLMChain, result: Dict, task_description: str, task_list: List[str], objective: str) -> List[Dict]:\n",
        "    \"\"\"Get the next task.\"\"\"\n",
        "    incomplete_tasks = \", \".join(task_list)\n",
        "    response = task_creation_chain.run(result=result, task_description=task_description, incomplete_tasks=incomplete_tasks, objective=objective)\n",
        "    new_tasks = response.split('\\n')\n",
        "    return [{\"task_name\": task_name} for task_name in new_tasks if task_name.strip()]\n",
        "\n",
        "def prioritize_tasks(task_prioritization_chain: LLMChain, this_task_id: int, task_list: List[Dict], objective: str) -> List[Dict]:\n",
        "    \"\"\"Prioritize tasks.\"\"\"\n",
        "    task_names = [t[\"task_name\"] for t in task_list]\n",
        "    next_task_id = int(this_task_id) + 1\n",
        "    response = task_prioritization_chain.run(task_names=task_names, next_task_id=next_task_id, objective=objective)\n",
        "    new_tasks = response.split('\\n')\n",
        "    prioritized_task_list = []\n",
        "    for task_string in new_tasks:\n",
        "        if not task_string.strip():\n",
        "            continue\n",
        "        task_parts = task_string.strip().split(\".\", 1)\n",
        "        if len(task_parts) == 2:\n",
        "            task_id = task_parts[0].strip()\n",
        "            task_name = task_parts[1].strip()\n",
        "            prioritized_task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n",
        "    return prioritized_task_list\n",
        "\n",
        "def _get_top_tasks(vectorstore, query: str, k: int) -> List[str]:\n",
        "    \"\"\"Get the top k tasks based on the query.\"\"\"\n",
        "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
        "    if not results:\n",
        "        return []\n",
        "    sorted_results, _ = zip(*sorted(results, key=lambda x: x[1], reverse=True))\n",
        "    return [str(item.metadata['task']) for item in sorted_results]\n",
        "\n",
        "def execute_task(vectorstore, execution_chain: LLMChain, objective: str, task: str, k: int = 5) -> str:\n",
        "    \"\"\"Execute a task.\"\"\"\n",
        "    context = _get_top_tasks(vectorstore, query=objective, k=k)\n",
        "    return execution_chain.run(objective=objective, context=context, task=task)\n",
        "\n",
        "class BabyAGI(Chain, BaseModel):\n",
        "    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n",
        "\n",
        "    task_list: deque = Field(default_factory=deque)\n",
        "    task_creation_chain: TaskCreationChain = Field(...)\n",
        "    task_prioritization_chain: TaskPrioritizationChain = Field(...)\n",
        "    execution_chain: ExecutionChain = Field(...)\n",
        "    task_id_counter: int = Field(1)\n",
        "    vectorstore: VectorStore = Field(init=False)\n",
        "    max_iterations: Optional[int] = None\n",
        "\n",
        "    class Config:\n",
        "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def add_task(self, task: Dict):\n",
        "        self.task_list.append(task)\n",
        "\n",
        "    def print_task_list(self):\n",
        "        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "        for t in self.task_list:\n",
        "            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n",
        "\n",
        "    def print_next_task(self, task: Dict):\n",
        "        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n",
        "\n",
        "    def print_task_result(self, result: str):\n",
        "        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "        print(result)\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        return [\"objective\"]\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        return []\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Run the agent.\"\"\"\n",
        "        objective = inputs['objective']\n",
        "        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n",
        "        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n",
        "        num_iters = 0\n",
        "        while True:\n",
        "            if self.task_list:\n",
        "                self.print_task_list()\n",
        "\n",
        "                # Step 1: Pull the first task\n",
        "                task = self.task_list.popleft()\n",
        "                self.print_next_task(task)\n",
        "\n",
        "                # Step 2: Execute the task\n",
        "                result = execute_task(\n",
        "                    self.vectorstore, self.execution_chain, objective, task[\"task_name\"]\n",
        "                )\n",
        "                this_task_id = int(task[\"task_id\"])\n",
        "                self.print_task_result(result)\n",
        "\n",
        "                # Step 3: Store the result in Pinecone\n",
        "                result_id = f\"result_{task['task_id']}\"\n",
        "                self.vectorstore.add_texts(\n",
        "                    texts=[result],\n",
        "                    metadatas=[{\"task\": task[\"task_name\"]}],\n",
        "                    ids=[result_id],\n",
        "                )\n",
        "\n",
        "                # Step 4: Create new tasks and reprioritize task list\n",
        "                new_tasks = get_next_task(\n",
        "                    self.task_creation_chain, result, task[\"task_name\"], [t[\"task_name\"] for t in self.task_list], objective\n",
        "                )\n",
        "                for new_task in new_tasks:\n",
        "                    self.task_id_counter += 1\n",
        "                    new_task.update({\"task_id\": self.task_id_counter})\n",
        "                    self.add_task(new_task)\n",
        "                self.task_list = deque(\n",
        "                    prioritize_tasks(\n",
        "                        self.task_prioritization_chain, this_task_id, list(self.task_list), objective\n",
        "                    )\n",
        "                )\n",
        "            num_iters += 1\n",
        "            if self.max_iterations is not None and num_iters == self.max_iterations:\n",
        "                print(\"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "                break\n",
        "        return {}\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(\n",
        "        cls,\n",
        "        llm: BaseLLM,\n",
        "        vectorstore: VectorStore,\n",
        "        verbose: bool = False,\n",
        "        **kwargs\n",
        "    ) -> \"BabyAGI\":\n",
        "        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n",
        "        task_creation_chain = TaskCreationChain.from_llm(\n",
        "            llm, verbose=verbose\n",
        "        )\n",
        "        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n",
        "            llm, verbose=verbose\n",
        "        )\n",
        "        execution_chain = ExecutionChain.from_llm(llm, verbose=verbose)\n",
        "        return cls(\n",
        "            task_creation_chain=task_creation_chain,\n",
        "            task_prioritization_chain=task_prioritization_chain,\n",
        "            execution_chain=execution_chain,\n",
        "            vectorstore=vectorstore,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "# Define your embedding model\n",
        "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "# Initialize the vectorstore as empty\n",
        "embedding_size = 1536\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}, relevance_score_fn=relevance_score_fn)\n",
        "\n",
        "\n",
        "# Logging of LLMChains\n",
        "verbose=False\n",
        "# If None, will keep on going forever\n",
        "max_iterations: Optional[int] = 3\n",
        "baby_agi = BabyAGI.from_llm(\n",
        "    llm=LLM,\n",
        "    vectorstore=vectorstore,\n",
        "    verbose=verbose,\n",
        "    max_iterations=max_iterations\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y5Qho9Zhy2b5",
        "outputId": "ef7b8c15-cb6c-4cde-b160-65690286aa76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the chat application!\n",
            "1. Start chatting\n",
            "2. Agent summaries\n",
            "3. Exit\n",
            "Enter your choice: 2\n",
            "Samantha's summary:\n",
            "Name: Samantha Williams (age: 35)\n",
            "Innate traits: detail-oriented, analytical\n",
            "No statements or information provided to summarize Samantha Williams's core characteristics.\n",
            "\n",
            "John's summary:\n",
            "Name: John Thompson (age: 40)\n",
            "Innate traits: compassionate, empathetic\n",
            "John Thompson's core characteristics are not specified in the given statements.\n",
            "\n",
            "Michelle's summary:\n",
            "Name: Michelle Chen (age: 32)\n",
            "Innate traits: patient, good communicator\n",
            "It is not possible to provide a summary as there are no statements provided about Michelle Chen's characteristics.\n",
            "\n",
            "Kevin's summary:\n",
            "Name: Kevin Smith (age: 45)\n",
            "Innate traits: analytical, trustworthy\n",
            "Kevin Smith is a filmmaker known for his comedic and irreverent style. He often incorporates pop culture references and uses dialogue-driven storytelling. He has a strong fan base and is active on social media, where he engages with his followers. He is also known for his outspokenness and willingness to discuss controversial topics.\n",
            "\n",
            "Welcome to the chat application!\n",
            "1. Start chatting\n",
            "2. Agent summaries\n",
            "3. Exit\n",
            "Enter your choice: 1\n",
            "Select an agent to chat with (or type 'menu' to go back to the main menu):\n",
            "1. Samantha\n",
            "2. John\n",
            "3. Michelle\n",
            "4. Kevin\n",
            "> 2\n",
            "\n",
            "You are now chatting with John Thompson. Use the following commands:\n",
            "exitnow - Go back to the agent selection\n",
            "/command main menu - Go back to the main menu\n",
            "exitall - Quit the application\n",
            "\n",
            "> hi there!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-75e114dffe75>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mrun_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-75e114dffe75>\u001b[0m in \u001b[0;36mrun_app\u001b[0;34m(agents)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mchat_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mprint_agent_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-75e114dffe75>\u001b[0m in \u001b[0;36mchat_loop\u001b[0;34m(agents)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Changed from 'interview_agent' to the 'generate_response' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{selected_agent.name} says: {response}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GenerativeAgent' object has no attribute 'generate_response'"
          ]
        }
      ],
      "source": [
        "#@title Chat Controller\n",
        "#@title Chat Controller\n",
        "from typing import Dict, Optional\n",
        "\n",
        "class GenerativeAgent:  # Add a placeholder class definition for GenerativeAgent\n",
        "    def __init__(self):\n",
        "        self.name = \"Agent\"\n",
        "\n",
        "    def get_summary(self):\n",
        "        return \"Summary of the agent.\"\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        return f\"Response to: {user_input}\"\n",
        "\n",
        "\n",
        "def print_agent_summaries(agents: Dict[str, GenerativeAgent]) -> None:\n",
        "    \"\"\"Prints the summaries of all agents.\"\"\"\n",
        "    for agent_name, agent in agents.items():\n",
        "        print(f\"{agent_name}'s summary:\")\n",
        "        print(agent.get_summary())\n",
        "        print()\n",
        "\n",
        "def select_agent(agents: Dict[str, GenerativeAgent]) -> Optional[GenerativeAgent]:\n",
        "    \"\"\"Lets the user choose an agent from the list or go back to the main menu.\"\"\"\n",
        "    print(\"Select an agent to chat with (or type 'menu' to go back to the main menu):\")\n",
        "    for i, (agent_name, agent) in enumerate(agents.items(), start=1):\n",
        "        print(f\"{i}. {agent_name}\")\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"> \")\n",
        "        if choice.lower() == \"menu\":\n",
        "            return None\n",
        "        elif choice.isdigit() and 1 <= int(choice) <= len(agents):\n",
        "            return list(agents.values())[int(choice) - 1]\n",
        "        else:\n",
        "            print(\"Invalid selection. Please enter a number corresponding to the agent or type 'menu' to go back to the main menu.\")\n",
        "\n",
        "\n",
        "def chat_loop(agents: Dict[str, GenerativeAgent]) -> None:\n",
        "    while True:\n",
        "        selected_agent = select_agent(agents)\n",
        "\n",
        "        if selected_agent is None:\n",
        "            break\n",
        "\n",
        "        print(f\"\\nYou are now chatting with {selected_agent.name}. Use the following commands:\\n\"\n",
        "              f\"exitnow - Go back to the agent selection\\n\"\n",
        "              f\"/command main menu - Go back to the main menu\\n\"\n",
        "              f\"exitall - Quit the application\\n\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"> \")\n",
        "            if user_input.lower() == \"exitnow\":\n",
        "                break\n",
        "            elif user_input.lower() == \"/command main menu\":\n",
        "                return\n",
        "            elif user_input.lower() == \"exitall\":\n",
        "                return\n",
        "\n",
        "            response = selected_agent.generate_response(user_input)  # Changed from 'interview_agent' to the 'generate_response' method\n",
        "            print(f\"{selected_agent.name} says: {response}\\n\")\n",
        "\n",
        "def run_app(agents: Dict[str, GenerativeAgent]) -> None:\n",
        "    while True:\n",
        "        print(\"Welcome to the chat application!\")\n",
        "        print(\"1. Start chatting\")\n",
        "        print(\"2. Agent summaries\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice: \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            chat_loop(agents)\n",
        "        elif choice == \"2\":\n",
        "            print_agent_summaries(agents)\n",
        "        elif choice == \"3\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\\n\")\n",
        "\n",
        "\n",
        "run_app(agents_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oxvv3y2xCu-T"
      },
      "outputs": [],
      "source": [
        "#@title Simulate Conversation Between Different Agents\n",
        "def run_conversation(agents: List[GenerativeAgent], initial_observation: str) -> None:\n",
        "    \"\"\"Runs a conversation between agents.\"\"\"\n",
        "    _, observation = agents[1].generate_reaction(initial_observation)\n",
        "    print(observation)\n",
        "    turns = 0\n",
        "    while True:\n",
        "        break_dialogue = False\n",
        "        for agent in agents:\n",
        "            stay_in_dialogue, observation = agent.generate_dialogue_response(observation)\n",
        "            print(observation)\n",
        "            # observation = f\"{agent.name} said {reaction}\"\n",
        "            if not stay_in_dialogue:\n",
        "                break_dialogue = True\n",
        "        if break_dialogue:\n",
        "            break\n",
        "        turns += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lcJVEOmiYFkO"
      },
      "outputs": [],
      "source": [
        "#@title Print all of the agent summaries\n",
        "def print_agent_summaries(agents: List[GenerativeAgent]) -> None:\n",
        "    \"\"\"Prints the summaries of all agents.\"\"\"\n",
        "    for agent in agents:\n",
        "        print(f\"{agent.name}'s summary:\")\n",
        "        print(agent.get_summary())\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO9pvnTMC0S-"
      },
      "outputs": [],
      "source": [
        "agents = [jordan, tommie]\n",
        "run_conversation(agents, \"Jordan said: Have you seen the rabid racoon killing people next door!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NLD_gnaPyruh"
      },
      "outputs": [],
      "source": [
        "#@title Tommy Summary\n",
        "#Now that Tommie has 'memories', their self-summary is more descriptive, though still rudimentary.\n",
        "# We will see how this summary updates after more observations to create a more rich description.\n",
        "print(tommie.get_summary(force_refresh=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhB9GqU9xvKQ"
      },
      "outputs": [],
      "source": [
        "def interview_agent(agent: GenerativeAgent, message: str) -> str:\n",
        "    \"\"\"Help the notebook user interact with the agent.\"\"\"\n",
        "    new_message = f\"{USER_NAME} says {message}\"\n",
        "    return agent.generate_dialogue_response(new_message)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pgJzk-aznWD"
      },
      "outputs": [],
      "source": [
        "interview_agent(tommie, \"'/get_task_list'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hVEjXjHCha1"
      },
      "outputs": [],
      "source": [
        "OBJECTIVE = \"List what we have talked about so far\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx3mcsSlC7FM"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0,openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2BtcSKUClmU"
      },
      "outputs": [],
      "source": [
        "# Logging of LLMChains\n",
        "verbose=False\n",
        "# If None, will keep on going forever\n",
        "embedding_size = 1536\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}, relevance_score_fn=relevance_score_fn)\n",
        "max_iterations: Optional[int] = 3\n",
        "baby_agi = BabyAGI.from_llm(\n",
        "    llm=llm,\n",
        "    vectorstore=vectorstore,\n",
        "    verbose=verbose,\n",
        "    max_iterations=max_iterations\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_KMg__DFgmW"
      },
      "outputs": [],
      "source": [
        "baby_agi({\"objective\": OBJECTIVE})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjIicponeT36sD84540QFv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}